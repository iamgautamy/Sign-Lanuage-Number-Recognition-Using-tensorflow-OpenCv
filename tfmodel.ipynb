{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "#Creating the dimensions for the ROI...\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _ , thresholded = cv2.threshold(diff, threshold,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    # Grab the external contours for the image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(),\n",
    "    cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam = cv2.VideoCapture(1)\n",
    "num_frames = 0\n",
    "element =9\n",
    "num_imgs_taken = 0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    # flipping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = frame.copy()\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "(80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            \n",
    "    #Time to configure the hand specifically into the ROI...\n",
    "    elif num_frames <= 300: \n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" +\n",
    "        str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "        (0,0,255),2)\n",
    "        \n",
    "        # Checking if the hand is actually detected by counting the number of contours detected...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "            ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element),\n",
    "            (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "            ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\"\n",
    "      + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "      (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 300:\n",
    "                cv2.imwrite(r\"D:\\\\self_project_frameworks\\\\hand action rec\\\\new\\\\gestures\\\\\"+str(element)+\"\\\\\" +\n",
    "                str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400),\n",
    " cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "# Releasing the camera & destroying all the windows...\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = r\"D:\\self_project_frameworks\\hand action rec\\new\\gestures\"\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('gestures', output=\"output\", seed=1337, ratio=(.8, 0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6498 images belonging to 9 classes.\n",
      "Found 819 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'D:\\self_project_frameworks\\hand action rec\\new\\output\\train'\n",
    "test_path = r'D:\\self_project_frameworks\\hand action rec\\new\\output\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches)\n",
    "import matplotlib.pyplot as plt\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(9,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "650/650 [==============================] - 105s 161ms/step - loss: 0.4092 - accuracy: 0.9109 - val_loss: 0.0221 - val_accuracy: 0.9939 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "650/650 [==============================] - 66s 101ms/step - loss: 0.0062 - accuracy: 0.9992 - val_loss: 0.0030 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "650/650 [==============================] - 63s 97ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0022 - val_accuracy: 0.9988 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "650/650 [==============================] - 63s 98ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "650/650 [==============================] - 65s 100ms/step - loss: 7.8820e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "650/650 [==============================] - 49s 76ms/step - loss: 6.1696e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "650/650 [==============================] - 60s 93ms/step - loss: 5.1836e-04 - accuracy: 1.0000 - val_loss: 9.9247e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "650/650 [==============================] - 56s 86ms/step - loss: 4.7362e-04 - accuracy: 1.0000 - val_loss: 9.7215e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "650/650 [==============================] - 42s 65ms/step - loss: 4.3560e-04 - accuracy: 1.0000 - val_loss: 8.5550e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "650/650 [==============================] - 42s 64ms/step - loss: 4.0017e-04 - accuracy: 1.0000 - val_loss: 9.4701e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.0001537102070869878; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "# For getting next batch of testing imgs...\n",
    "imgs, labels = next(test_batches) \n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#Once the model is fitted we save the model using model.save()  function.\n",
    "\n",
    "\n",
    "model.save('hand_model_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "five four one four five three one five one eight "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "five   four   one   four   five   three   one   five   one   eight   "
     ]
    }
   ],
   "source": [
    "word_dict = {0:'one', 1:'two', 2:'three', 3:'four', 4:'five', 5:'six', 6:'seven', 7:'eight', 8:'nine'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)],end=' ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"D:\\self_project_frameworks\\hand action rec\\new\\hand_model_new.h5\")\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    \n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255,\n",
    "    cv2.THRESH_BINARY)\n",
    "    \n",
    "     #Fetching contours in the frame (These contours can be of hand\n",
    "    #or any other object in foreground) …\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If length of contours list = 0, means we didn't get any#contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Returning the hand segment(max contour) and the\n",
    "  #thresholded image of hand...\n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(1)\n",
    "num_frames =0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flipping the frame to prevent inverted image of captured\n",
    "    #frame...\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # ROI from the frame\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "\n",
    "    if num_frames < 70:\n",
    "        \n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "  (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "    else: \n",
    "        # segmenting the hand region\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,\n",
    "      ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "            \n",
    "            thresholded = cv2.resize(thresholded, (64, 64))\n",
    "            thresholded = cv2.cvtColor(thresholded,\n",
    " cv2.COLOR_GRAY2RGB)\n",
    "            thresholded = np.reshape(thresholded,\n",
    "(1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "            \n",
    "            pred = model.predict(thresholded)\n",
    "            cv2.putText(frame_copy, word_dict[np.argmax(pred)],\n",
    "(170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    # Draw ROI on frame_copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,\n",
    "    ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    # incrementing the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\",\n",
    "    (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "    # Close windows with Esc\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7044b324f88c4208d441c8fce3c8f6c454026eeaf7e810a2852062e3590255dc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
